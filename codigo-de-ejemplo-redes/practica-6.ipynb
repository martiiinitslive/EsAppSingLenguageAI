{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Practica 6\n\nEn esta práctica entrenaremos un nuevo clasificador de audio. En la práctica 5\ndiseñamos un total de 85 características a partir del espectrograma. En esta práctica\ndejaremos que sea la red neuronal la que aprenda directamente a extraer estas características\na partir del espectrograma mel. \n\nComenzaremos por las funciones necesarias para preprocesar los audios. Esto es, pasar\nde las muestras originales al espectrograma mel. \n\n\n## Preprocesado de audio\n\nEl preprocesado de audio debe obtener el espectrograma de cada audio a partir de las muestras\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport librosa\nimport numpy as np\nfrom utils import nextpow2\nimport matplotlib.pyplot as plt\nimport pathlib\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom utils import detect_speech\nimport pytest\nimport UPVlog\nnotebook_filename=\"practica6\"\nmy_logger=UPVlog.UPVlog(notebook_filename)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"La función siguiente debe extraer el espectrograma a partir de un fichero de audio. Los pasos serán los siguientes:\n\n1.  Cargar el fichero wav\n2.  Recortar los silencios iniciales y finales con `detect_speech`\n3.   Si la duracion del audio recortado es menor que 1.1 segundos. Añadiremos ceros al final hasta completar esa longitud. El objetivo es que el todos los audios preprocesados tengan la misma duración. En caso de que la duración del audio recortado sea mayor que 1.1 segundos recortaremos el audio quedandonos con los primeros 1.1 segundos.\n4.    Calculo del espectrograma mel con los parametros indicados en la función\n\n","metadata":{}},{"cell_type":"code","source":"def linear_normalize(array):\n    min_val = np.min(array)\n    max_val = np.max(array)\n    normalized_array = (array - min_val) / (max_val - min_val + 1e-10)\n    return normalized_array\n\n\ndef preprocess_audio(filename):\n    \n    #Cargamos audio y recortamos silencios\n    \n    audio_data, sample_rate = librosa.load(filename, sr=None)\n    \n    audio_mask = detect_speech(audio_data, sample_rate)    \n\n    audio_trim = audio_data[audio_mask>0]\n    \n    if len(audio_trim)< 0.1 * sample_rate :# detect speech no ha ido bien, descartamos audio\n        return None\n    \n    target_duration = 1.1 # para que todos los audios duren lo mismo\n    target_length = round(target_duration * sample_rate)\n\n    if len(audio_trim) > target_length:\n    # si la duracion es mayor que target length recortamos y si es menor rellenamos con ceros\n    # Dejar el resultado en la variable audio\n        audio = audio_trim[:target_length] # dejar el resultado en esta variable\n    else:\n        padding = target_length - len(audio_trim)\n        zeros = np.zeros(padding, dtype=audio_trim.dtype)\n        audio = np.concatenate([audio_trim, zeros])\n        #o esto:\n        # audio = np.pad(audio_trim, (0, padding), mode='constant')\n        # hace lo mismo q np.zeros y np.concentrate pero todo en una linea\n    \n    n_mels = 32\n    frameDuration = 0.03 # 30 ms\n    win_length = int(frameDuration*sample_rate)\n    n_fft = 2**nextpow2(win_length)\n    \n    #Calcula el espectrograma mel con los parametros anteriores\n    hop_length = win_length//2\n\n    S = librosa.feature.melspectrogram(\n        \n        y = audio,\n        sr = sample_rate,\n        n_fft = n_fft,\n        hop_length=hop_length,\n        win_length=win_length,\n        n_mels=n_mels,\n        power=2.0 \n    )\n\n    # Convert to decibels (log scale)\n    S_dB = np.log(S+1e-6)\n\n    S_dB = linear_normalize(S_dB) # Ajusta las amplitudes entre [0-1]\n\n    return S_dB\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Comprobación\nmy_logger.test(\"preprocesado audio\")\nS_dB = preprocess_audio('siete.wav')\n#S_dB = preprocess_audio('/srv/data/audios/base_datos_numeros/train/seis/2023_008_AB_seis_4_1.wav')\nassert S_dB.shape == (32,74), \"comprueba como calculas el espectrograma mel\"\n#assert S_dB[10,10]==pytest.approx(0.30, abs=1e-2)\n\nprint(S_dB.shape)\n\n# Plot the Mel spectrogram\nsample_rate = 16000\nlibrosa.display.specshow(S_dB, sr=sample_rate,  hop_length=240, x_axis='time', y_axis='mel')\nplt.title('Mel-frequency spectrogram')\nplt.tight_layout()\nmy_logger.success(\"preprocesado audio\",4)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create dataset\n\nAhora crearemos los datasets para realizar el entrenamiento de la red neuronal. \n\nEn primer lugar vamos a \npreprocesar todos los audios de la base de datos. Obteniendo como resultado \ndos tensores con todas las imagenes de train y validacion respectivamente. \n\n","metadata":{}},{"cell_type":"code","source":"#localizacion de los wav\naudios_train = '/srv/data/audios/base_datos_numeros/train'\naudios_valid = '/srv/data/audios/base_datos_numeros/test/'\n\n#las etiquetas son el nombre de la carpeta\nmap_labels = {'cero': 0,\n              'uno': 1,\n              'dos': 2,\n              'tres': 3,\n              'cuatro':4,\n              'cinco':5,\n              'seis':6,\n              'siete':7,\n              'ocho':8,\n              'nueve':9}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Para crear los datasets vamos a emplear la funcion `create_dataset`. Los pasos que realiza son:\n1. Crea una lista con todos los ficheros wav\n2. Los guarda en una tabla de pandas (parecido a una hoja excel)\n3. Crea columna `label`, a partir del nombre de la carpeta de cada audio\n4. Convierte `label` a un entero con el diccionario `map_labels`.\n5. Para cada fichero extrae el espectrograma con la funcion `preprocess_audio`.\n6. Crea un tensor con los espectrogramas y un tensor con las etiquetas\n7. Con la clase `TensorDataset` de Pytorch crea el dataset.","metadata":{}},{"cell_type":"code","source":"\ndef create_dataset(audios_folder):\n    audio_files = pathlib.Path(audios_folder).rglob('*.wav')\n    df = pd.DataFrame({'filename': list(audio_files)})\n    df['label']=df.filename.apply(lambda x: str(x).split('/')[-2]) \n    df['y'] = df.label.map(map_labels)\n    print(\"Number of  images  in dataset= \", len(df))\n    #print(df.head())\n\n    all_specgrams = []\n    all_labels = []\n\n    num_audios = len(df)\n\n    for k in tqdm(range( num_audios)):\n        row = df.iloc[k]\n        features_v = preprocess_audio(row['filename'])\n        if features_v is None:\n            continue\n\n        if features_v.shape[1] < 10:\n            print(\"bad audio \", row['filename'])\n            continue\n        all_labels.append(row['y'])\n        all_specgrams.append(features_v[np.newaxis,:,:])\n        #if k > 10:\n        #    break\n    all_specgrams = np.array(all_specgrams)\n    all_labels = np.array(all_labels)\n\n    X_tensor = torch.tensor(all_specgrams, dtype=torch.float32)\n    Y_tensor = torch.tensor(all_labels, dtype=torch.long)\n\n    # Create TensorDataset\n    dataset = TensorDataset(X_tensor, Y_tensor)\n\n    return dataset\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# creamos tanto el dataset de train como el de validacion\n\ntrain_dataset = create_dataset(audios_train)\nval_dataset = create_dataset(audios_valid)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Antes de continuar es conveniente visualizar algunos ejemplos del datast para ver que todo es correcto","metadata":{}},{"cell_type":"code","source":"def show_samples(dataset, nrows=9, ncols = 3):\n    fig, ax = plt.subplots(nrows,ncols, figsize = ( ncols*4, nrows*4))\n    axl = ax.flatten()\n    \n    num_samples = len(train_dataset)\n    for k in range(len(axl)):\n        sample_idx = np.random.randint(0,num_samples)\n        spec, label = dataset[sample_idx]\n        librosa.display.specshow(spec[0].numpy(), sr=16000, hop_length=240, x_axis='time', y_axis='mel',ax= axl[k])\n        #axl[k].imshow(spec[0])\n        axl[k].set_title(f\"label {label}\")\n    plt.subplots_adjust(hspace=0.3, wspace=0.3)\n\nshow_samples(train_dataset)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Network\n\nEn este apartado definiremos la red neuronal convolucional. Al igual que en la practica 5, nos definiremos un modulo auxiliar \nque se repite varias veces en la red en concreto el módulo `conv_layer`. Debe añadir los siguientes elementos:\n* `nn.Conv2d`: Convolucion 2d, kernel de tamaño 3 y padding =1\n* `nn.BatchNorm2d`: Similar al batch norm para el caso de imagenes\n* `nn.ReLU`: No linealidad\n\nComo verá en el codigo de abajo, opcionalmente este modulo tambien puede realizar max pooling.\n","metadata":{}},{"cell_type":"code","source":"from torch import nn\n\nclass conv_layer(nn.Sequential):\n    def __init__(self,input_feat, out_feat, max_pool = True):\n        layers = []\n        #Complete con vonv2d, batchnorm2d y relu añadiendo a la lista: layers.append(...)\n        \n        # 1) Convolución 2d, kernel=3, padding=1\n        layers.append(nn.Conv2d(in_channels=input_feat,\n                                out_channels=out_feat,\n                                kernel_size=3,\n                                padding=1))\n        # 2) BatchNorm2d\n        layers.append(nn.BatchNorm2d(num_features=out_feat))\n        # 3) ReLU\n        layers.append(nn.ReLU(inplace=True))\n        \n        if max_pool:\n            layers.append(nn.MaxPool2d(3,2, padding=1))\n        super().__init__(*layers)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Comprobacion\nmy_logger.test(\"conv layer\")\n\n\nl = conv_layer(1,16)\n\nelems = list(l.children())\nassert isinstance(elems[0],nn.Conv2d), \"primero conv2d\"\nassert isinstance(elems[1], nn.BatchNorm2d), \"primero conv2d\"\nassert isinstance(elems[2], nn.ReLU), \"primero conv2d\"\nassert elems[0].in_channels == 1, \"nuermo de canales de entrada mal\"\nassert elems[0].out_channels == 16, \"nuermo de canales de entrada mal\"\nmy_logger.success(\"conv layer\",3)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Modulo final antes de la salida, no tiene parametros por tanto no hace falta init\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return torch.flatten(x, start_dim=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ahora crearemos la red convolucional con los modulos anteriores, a estructura es la siguiente (tomaremos numF=16):\n* Capa1, conv_layer input_channels->numF, con maxpool\n* Capa2, conv_layer numF -> 2* numF, con maxpool\n* Capa3, conv_layer 2*numF -> 4* numF, con maxpool\n* Capa4, conv_layer 4*numF -> 4* numF, **sin** maxpool\n* Capa5, conv_layer 4*numF -> 4* numF, **sin** maxpool\n* Maxpool temporal (se proporciona)\n* Dropout (se proporciona)\n* Flatten (se proporciona)\n* Lineal de salida (se proporciona)\n\n\n","metadata":{}},{"cell_type":"code","source":"     \nclass MyNet(nn.Sequential):\n    def __init__(self,dropoutProb = 0.2, num_classes = 10):\n        input_channels = 1\n        numF = 16\n        # Tamaño del último maxPooling. Hace que la última capa solo tenga 1\n        # neurona en la dimensión temporal\n        timePoolSize = int(np.ceil(74/8))\n\n        layers = []\n        # añada las capas que se le indica en el texto\n        layers.append(conv_layer(input_channels, numF, max_pool=True))\n\n        layers.append(conv_layer(numF, 2 * numF, max_pool=True))\n\n        layers.append(conv_layer(2 * numF, 4 * numF, max_pool=True))\n\n        layers.append(conv_layer(4 * numF, 4 * numF, max_pool=False))\n\n        layers.append(conv_layer(4 * numF, 4 * numF, max_pool=False))\n\n\n        layers.append(nn.MaxPool2d((1,timePoolSize)))  \n        layers.append(nn.Dropout2d(dropoutProb))\n        layers.append(Flatten())\n        layers.append(nn.Linear(64*4,num_classes)) #obtiene los logits\n        \n\n        super().__init__(*layers)\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"my_logger.test(\"conv net\")\n\nX_test = torch.randn(1,1,32,74)\nnet = MyNet()\ny = net(X_test)\nassert y.shape == (1,10), \"tendremos una salida\"\nassert len(list(net.modules()))==28, \"revise la estructura de la red\"\nassert sum([len(p) for p in net.parameters()]) == 980, \"revise la estructura de la red\"\nmy_logger.success(\"conv net\",3)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Loop\n\nUna vez definida la red la parte de entrenamiento es exactamente igual que la de la practica 5. \n\nEn el caso de las redes convolucionales la GPU si está disponible supone una gran diferencia para entrenar los modelos. \n\nComente la linea 'cpu' o 'cuda' y compruebe la diferencia abismal entre los tiempos de ejecución:","metadata":{}},{"cell_type":"code","source":"from train_net import train\nimport torch.optim as optim\nimport time\n\n#device = 'cpu'\ndevice = 'cuda'\n\n# Define learning rate\nlearning_rate = 0.001\nbatch_size = 512\n\n# Define network\nred = MyNet()\n\n# Create Adam optimizer\n#optimizer = optim.SGD(red.parameters(), lr=learning_rate, momentum =0.9)\noptimizer = optim.Adam(red.parameters(), lr=learning_rate)\nloss_fn = nn.CrossEntropyLoss()\n\n\nnum_epochs = 30\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\nstart_time = time.time()\ntrain_losses, val_losses, train_acc, val_acc = train(red, \n                                                     loss_fn, \n                                                     train_dataloader,val_dataloader, \n                                                     optimizer, num_epochs, device=device)\n\nprint(f\"Train finished in {time.time()- start_time} seconds\") \n\n\nfig, axes = plt.subplots(2,1)\naxes[0].plot(train_losses, label='train loss')\naxes[0].plot(val_losses, label = 'val_loss')\naxes[0].legend()\naxes[0].set_xlabel('epoch')\naxes[0].set_ylabel('Loss')\n\naxes[1].plot(train_acc, label='train_acc')\naxes[1].plot(val_acc, label='val acc')\naxes[1].legend()\naxes[1].set_xlabel('epoch')\naxes[1].set_ylabel('Accuracy')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prueba con grabación\n\nTomaremos el modelo del apartado anterior para hacer nuestras pruebas con audios grabados\nal instante.\n\n","metadata":{}},{"cell_type":"code","source":"from ipywebrtc import AudioRecorder, CameraStream\nfrom IPython.display import Audio\nimport librosa\n\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Ejecuta la siguiente celda.\n* Pulsa circulo negro para comenzar a grabar.\n* Di un numero\n* Pulsa de nuevo para terminar la grabacion","metadata":{}},{"cell_type":"code","source":"camera = CameraStream(constraints={'audio': True,'video':False})\nrecorder = AudioRecorder(stream=camera)\nrecorder","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# convertimos a wav\nwith open('recording.webm', 'wb') as f:\n    f.write(recorder.audio.value)\n!ffmpeg -i recording.webm -ac 1 -ar 16000 -f wav file.wav  -y -hide_banner -loglevel panic","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Escucha para comprobar que todo sigue ok\nAudio('file.wav')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pintamos para ver el aspecto de la señal  que no sature \naudio_data, fs = librosa.load('file.wav', sr=None)\nt = np.arange(len(audio_data))/fs\nplt.plot(t,audio_data)\n_ = plt.xlabel('time')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"S =  preprocess_audio('file.wav')\n\nlibrosa.display.specshow(S, sr=16000, hop_length=240, x_axis='time', y_axis='mel')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"red.eval()\n\nx = torch.from_numpy(S.astype(np.float32))\nx = x.unsqueeze(0).unsqueeze(0) # añadimos dimensiones para poder hacer la inferencia\nx = x.to(device)\ny = red(x)\nprint(\"Has dicho: \", torch.argmax(y).item()) # clasificamos con el máximo","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.bar(np.arange(10),F.softmax(y,dim=-1).cpu().detach().numpy().flatten())\nplt.xlabel('Numero')\nplt.ylabel('Probabilidad')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}