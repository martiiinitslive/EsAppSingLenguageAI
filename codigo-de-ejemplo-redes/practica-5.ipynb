{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8236682",
   "metadata": {
    "papermill": {
     "duration": 0.01319,
     "end_time": "2025-06-13T12:55:20.874722",
     "exception": false,
     "start_time": "2025-06-13T12:55:20.861532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Practica 5\n",
    "El objetivo de esta práctica es el de entrenar un clasificador basado en red neuronal fully connected empleando las características de cada audio obtenidas al final de la práctica 4.\n",
    "\n",
    "Para la implementación y entrenamiento de la red neuronal emplearemos la libreria [pytorch](https://pytorch.org/). Pytorch tiene un manejo de tensores muy similar a numpy pero tiene dos ventajas importantes:\n",
    "* Permite la diferenciación (cálculo del gradiente) de forma automática.\n",
    "* Permite el uso de la GPU de forma muy sencilla, lo cual puede acelerar los tiempos de entrenamiento e inferencia de forma considerable.\n",
    "\n",
    "La mejor forma de introducirse en el manejo de Pytorch es consultar algunos de los tutoriales de ayuda disponibles en: [Tutoriales](https://pytorch.org/tutorials/).\n",
    "\n",
    "## Diferenciación automática\n",
    "\n",
    "Quizás esta es la carácterística más importante que hace de Pytorch una herramienta tan potente. \n",
    "\n",
    "Consideremos que $X \\in \\mathbb{R}^n$ es un vector de características y  $W \\in \\mathbb{R}^n$ es un vector con unos pesos y calculamos la siguiente expresión:abs\n",
    "\n",
    "$y = W^T * X$ \n",
    "\n",
    "Es decir el producto escalar de ambos vectores. Para calcular el gradiente $\\nabla y = [\\delta y/ \\ \\delta x_0, \\delta y/ \\ \\delta x_1, \\ldots \\delta y/ \\ \\delta x_{n-1} ] \\$, lo podemos hacer de la siguiente forma:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87869aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T12:55:20.897560Z",
     "iopub.status.busy": "2025-06-13T12:55:20.897248Z",
     "iopub.status.idle": "2025-06-13T12:55:26.861597Z",
     "shell.execute_reply": "2025-06-13T12:55:26.860226Z"
    },
    "papermill": {
     "duration": 5.976998,
     "end_time": "2025-06-13T12:55:26.863281",
     "exception": true,
     "start_time": "2025-06-13T12:55:20.886283",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'UPVlog'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/637636015.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mUPVlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'UPVlog'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pytest\n",
    "import UPVlog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e24c9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.random.randn(5) # vector de entrada aleatorio\n",
    "W = np.random.randn(5) # vector de pesos aleatorio\n",
    "y = np.dot(X, W) # producto escalar\n",
    "\n",
    "# Convertimos de numpy a torch\n",
    "Xt = torch.from_numpy(X)\n",
    "Wt = torch.from_numpy(W)\n",
    "yt = torch.dot(Xt,Wt)\n",
    "\n",
    "print(f\"{X=}\")\n",
    "print(f\"{Xt=}\")\n",
    "print(\"-\"*10)\n",
    "print(f\"{W=}\")\n",
    "print(f\"{Wt=}\")\n",
    "print(\"-\"*10)\n",
    "\n",
    "print(f\"resultado con numpy = f{y}\")\n",
    "print(f\"resultado con torch = f{yt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10504901",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Como vemos el resultado es identico empleando numpy o Pytorch. Además el paso de tensores de una librería a la otra es inmediato. \n",
    "\n",
    "Si quisieramos calcular el gradiente empleando numpy, deberíamos programar explicitamente la función. Este paso podría ser relativamente complejo en el caso de que la función fuera relativamente compleja (además de que sería bastante común la introducción de errores en la programación). \n",
    "\n",
    "Sin embargo, Pytorch nos ofrece una forma mucho más flexible de calcular los gradientes. Para ello: \n",
    "* Debemos indicar que tensores requieren el cálculo del gradiente.\n",
    "* Asegurarnos de que la variable sobre la que se cálcula el gradiene (en nuestro caso `y`, sea un escalar\n",
    "\n",
    "Entonces podemos hacer lo siguiente:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc7843",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# indicamos que queremos calcular el gradiente respecto de Wt\n",
    "Wt.requires_grad = True\n",
    "\n",
    "\n",
    "y2 = torch.dot(Wt, Xt) # realizamos la operacion que deseemos\n",
    "\n",
    "y2.backward() # calculamos las derivadas y aplicamos la regla de la cadena de forma recursiva\n",
    "\n",
    "\n",
    "print(f\"{y2=}\") # el esultado es igual que antes\n",
    "\n",
    "print(f\"El gradiente vale: {Wt.grad}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0115e28",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Como vemos el código anterior ha calculado el gradiente de forma automática. En este caso tan sencillo, el gradiente lo podemos calcular de forma exacta:abs\n",
    "\n",
    "$\\nabla y = X$\n",
    "\n",
    "Comprueba que efectivamente el valor del gradiente coincide con el vector `X`.\n",
    "\n",
    "Ahora vamos a añadir una función un poco más compleja que la anterior. La función correspondería con la evaluación de la función de coste de un clasificador lineal binario. Que tendría un grafo de operación como el de la siguiene imagen:\n",
    "\n",
    "![computational graph](figs/comp-graph.png)\n",
    "\n",
    "El código para implementar el grafo anterior sería. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023368d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "loss.backward() # calculamos gradientes\n",
    "\n",
    "print(f\"Gradiente de w: {w.grad}\") # tiene tantos elementos como w\n",
    "print(f\"Gradiente de b: {b.grad}\") # tiene tantos elementos como w\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e721ed",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Definición del dataset/dataloader\n",
    "\n",
    "Una vez vista la capacidad de Pytorch para calcular de forma automáticamente los gradientes. Pasaremos a la gestión de los datos. \n",
    "\n",
    "Para ello emplearemos las clases Dataset y Dataloader (ver este [enlace](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) para mas información).\n",
    "\n",
    "### Dataset\n",
    "\n",
    "El Dataset es una clase que permite organizar todos los datos disponibles para entrenar o validar. Cuando queremos crear un dataset específico, crearemos una subclase en la que reimplementaremos los tres metodos siguientes:\n",
    "\n",
    "* `def __init__(self, *args, **kwargs)`: función para indicar como debe obtener los datos\n",
    "* `def __len__(self)`: función para indicar cuantos datos tenemos\n",
    "* `def __getitem__(self, idx)`: Función que devuelve el elemento idx (numero entero) del dataset.\n",
    "\n",
    "El método más interesante de los anteriores es `__getitem__` que devuelve el ejemplo que ocupa la posición `idx`de train/validación.\n",
    "\n",
    "Vamos a crear un dataset con los vectores de características de los audios.  Pero antes de eso vamos a cargar y normalizar \n",
    "las características que tenemos precalculadas de todos los audios proporcionados por los alumnos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42da92",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = np.load('audio_features_train.npz')\n",
    "features_train = train_data['features']\n",
    "labels_train = train_data['labels']\n",
    "print(\"features_train shape\", features_train.shape)\n",
    "print(\"labels_train shape\", labels_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9acac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#repetimos para los de validacion\n",
    "valid_data = np.load('audio_features_val.npz')\n",
    "features_valid = valid_data['features']\n",
    "labels_valid = valid_data['labels']\n",
    "print(\"features_valid shape\", features_valid.shape)\n",
    "print(\"labels_valid shape\", labels_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea72e1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculamos media y std del train set\n",
    "# Es importante keepdims para poder hacer broadcasting\n",
    "m_train = features_train.mean(axis=0, keepdims = True)\n",
    "s_train = features_train.std(axis=0, keepdims = True)\n",
    "\n",
    "#normalizamos las features en ambos casos con la media y std de train\n",
    "features_train_norm = (features_train - m_train) / s_train\n",
    "features_valid_norm = (features_valid - m_train) / s_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ffa9d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class P5Dataset(Dataset): # heredamos de la clase Dataset \n",
    "    def __init__(self, features, labels):\n",
    "        #Simplemente guardamos las features y las labels\n",
    "        assert features.shape[1] == 85 #Comprobaciones\n",
    "        assert features.shape[0] == labels.shape[0]\n",
    "        self.features = features.astype(np.float32) # reducimos precision a float32\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0] # numero de ejemplos del dataset\n",
    "    def __getitem__(self, idx):\n",
    "        # devolvemos features, y label del ejemplo idx\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386aa05",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = P5Dataset(features_train_norm, labels_train)\n",
    "valid_dataset = P5Dataset(features_valid_norm, labels_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19461f0b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Vamos a pintar un ejemplo para comprobar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0828994",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_idx = 100\n",
    "\n",
    "features, label = train_dataset[sample_idx]\n",
    "\n",
    "plt.plot(features)\n",
    "plt.xlabel('features')\n",
    "plt.title(f'label = {label}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b124fab",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Podemos comprobar como tras la normalización los valores de todas las características aparecen en un rango dinámico parecido. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f5ff9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Dataloader\n",
    "\n",
    "El Dataset es util para organizar los datos, pero durante el entrenamiento de una red necesaria es necesario ir tomando grupos aleatorios de ejemplos de entrenamiento para formar los batches. En ocasiones la obtención de un ejemplo a partir del Dataset puede tardar cierto tiempo (por ejemplo en el caso de que los datos de entrenamiento sean imagenes). Por ello, la clase Dataloader se encarga de:\n",
    "* Seleccionar los ejemplos que van a formar un batch\n",
    "* Controlar las todas las iteraciones que serán necesarias para formar un epoch\n",
    "* Paralelizar la extracción de datos del Dataset\n",
    "\n",
    "Veamos como definir un dataset y como realizar un bucle para realizar un epoch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1817b47",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#necesitamos dataset, batch_size y si queremos shuffle (orden aleatorio)\n",
    "batch_size = 512\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb6865",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm # bucles bonitos\n",
    "\n",
    "for batch_feat, batch_labels in tqdm(train_dataloader): # genera un iterador\n",
    "    pass # en este bucle no hacemo nada todavia solo queremos ver el iterador "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d16099",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "En el resultado anterior podemos ver:\n",
    "* Que hemos completado una pasada por todos los datos (epoch)\n",
    "* El numero de batches que hemos empleado\n",
    "* La velocidas (iteraciones/s)\n",
    "\n",
    "Más abajo emplearemos los dataloaders para iterar por todos los ejemplos de entrenamiento y validación, pero de momento vamos a pasar a definir la red neuronal.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb221d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Primera red neuronal\n",
    "\n",
    "Una vez definidos los datasets y dataloaders para la gestión de los datos, vamos a definir la red neuronal. \n",
    "\n",
    "## Módulos en Pytorch\n",
    "\n",
    "Pytorch plantea una estructura modular para construir las redes neuronales. El bloque básico se llama [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html), este módulo implementa los siguientes métodos:\n",
    "* `__init__`: Es el constructor donde se indica como inicializar\n",
    "* `forward`: Este es el método donde se indica lo que debe hacer el módulo\n",
    "* `backward`: Para calcular los gradientes\n",
    "\n",
    "En la práctica construiremos nuevos módulos ensamblando módulos más simples. Para ello únicamente reimplementaremos los métodos `__init__` y `__forward__`, porque el método backward está gestionado por la clase padre `nn.Module`. \n",
    "\n",
    "Veamos un ejemplo, primero definiremos un módulo lineal (empleado en las capas de las redes fully connected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f5a85",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "input_dim = 10\n",
    "out_dim = 5\n",
    "\n",
    "# solo hay que indicar el número de neuronas de entrada y salida\n",
    "# La gestión de las variables que contienen los parametros la realiza\n",
    "# el módulo automáticamente\n",
    "mi_modulo = nn.Linear(input_dim, out_dim)\n",
    "\n",
    "#ejemplo de uso\n",
    "x = torch.randn(1,input_dim) # tiene que ser array bidimensional\n",
    "# el numero de filas indica el numero de ejemplos que procesamos simultáneamente\n",
    "\n",
    "y = mi_modulo(x) ## automticamente llama al metodo forward()\n",
    "\n",
    "print(\"La dimension 1 deberia ser out_dim:\",y.shape)\n",
    "assert y.shape[1] == 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79daff06",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    " ## Nuevos módulos\n",
    "Pytorch proporciona múltitud de módulos básicos que podemos combinar muy fácilmente para crear nuevos. En este ejemplo construiremos un nuevo modulo que:\n",
    "* Incluye capa lineal\n",
    "* Normaliza (batch norm)\n",
    "* Aplica no linealidad tipo ReLU\n",
    "\n",
    "Todos los nuevos modulos deben heredar de `nn.Module`:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2c997",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Capa(nn.Module): \n",
    "    def __init__(self, input_dim, out_dim): \n",
    "        super().__init__() #importante llamar al constructor del padre lo primero\n",
    "        \n",
    "        self.ff = nn.Linear(input_dim, out_dim) # capa lineal\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "    def forward(self, X): # aqui definimos lo que hara el nuevo modulo\n",
    "        y = self.ff(X)\n",
    "        y = self.bn(y)\n",
    "        y = self.relu(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f964d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Vamos a probar como usar nuestro nuevo modulo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc25497",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dim = 85\n",
    "out_dim = 10 \n",
    "\n",
    "capa = Capa(input_dim,out_dim)\n",
    "\n",
    "x = torch.randn(20,85) # 20 ejemplos random\n",
    "\n",
    "y = capa(x) # igual que antes, llama a forward\n",
    "\n",
    "print(\"Dimensiones y =\", y.shape)\n",
    "\n",
    "assert torch.all(y >= 0) # tras relu todos son >= 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d72f4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Una vez visto como generar un modulo ensablando varios vamos a construir nuevos modulos. \n",
    "\n",
    "Comenzaremos modificando el modulo `Capa`, que acabamos de construir para incluir una capa de [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) entre el `Batchnorm` y `ReLU`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc73d7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Este módulo incluirá dropout en el caso de que \n",
    "# p_dropout > 0.0. Por defecto no incluye dropout\n",
    "\n",
    "class Capa(nn.Module): \n",
    "    def __init__(self, input_dim, out_dim, p_dropout: float = 0.0): \n",
    "        super().__init__() #importante llamar al constructor del padre lo primero\n",
    "        \n",
    "        self.ff = nn.Linear(input_dim, out_dim) # capa lineal\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "        #Defina aqui el modulo dropout si p_dropout > 0\n",
    "        self.p_dropout = p_dropout # guardamos el valor para saber si hay que aplicarlo durante el forward\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        if p_dropout > 0:\n",
    "            self.dropout = nn.Dropout(p_dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    " \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "    def forward(self, X): # aqui definimos lo que hara el nuevo modulo\n",
    "        y = self.ff(X)\n",
    "        y = self.bn(y)\n",
    "        # aplique aqui dropout si self.p_dropout > 0\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        if self.dropout is not None:\n",
    "            y = self.dropout(y)        \n",
    "        \n",
    "        y = self.relu(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c24abe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprobación\n",
    "my_logger.test(\"Capa\")\n",
    "capa = Capa(85,10,p_dropout=0.2)\n",
    "\n",
    "assert len([m for m in capa.modules() if isinstance(m, nn.Dropout)]) == 1, \"no ha incluido el modulo dropout\"\n",
    "dropout_module = [m for m in capa.modules() if isinstance(m, nn.Dropout)][0]\n",
    "assert dropout_module.p == 0.2, \"Ajuste la probabilidad del modulo de dropout\"\n",
    "my_logger.success(\"Capa\",1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84bdcd3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Una red neuronal en Pytorch es simplemente objeto de la clase `nn.Module` más, pero que abarca desde la entrada hasta la salida. El siguiente ejemplo muestra la estructura de la red neuronal que emplearemos en esta práctica.\n",
    "\n",
    "En este caso, nuestra Red hereda de la clase [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).\n",
    "Simplemente tenemos que definir una lista de modulos que componen nuestra red para inicializar `nn.Sequential`. Lo interesante, es que no hay que redefinir el método `forward` porque ya está definido en la clase padre `nn.Sequential`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ebdef",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Red(nn.Sequential):\n",
    "    def __init__(self, input_dim=85, hidden_dim = [4,], out_dim = 10, p_dropout = 0.0):\n",
    "        assert len(hidden_dim)>0, \"at least one hidden layer\"\n",
    "        layers = [] #aqui guardamos una lista con todas las capas\n",
    "        #first layer\n",
    "        layers.append(Capa(input_dim,hidden_dim[0], p_dropout=p_dropout))\n",
    "\n",
    "        #Output layer\n",
    "        layers.append(nn.Linear(hidden_dim[-1], out_dim))\n",
    "        super().__init__(*layers) #inicializamos la clase padre con la lista de capas\n",
    "                \n",
    "                      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052a29c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mired = Red()\n",
    "print(mired)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf40ddb0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "La clase anterior crea una red neuronal con una capa oculta de 4 neuronas. El número de neuronas de la capa oculta viene definido en la variable `hidden_dim`. Como `hidden_dim` es una lista podríamos añadir nuevas capas ocultas añadiendo más elementos a la lista `hidden_dim`.\n",
    "\n",
    "Complete el código siguiente para que si `hidden_dim` tiene N elementos cree N capas ocultas cada una con el número de neuronas correspondiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb1562",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Red(nn.Sequential):\n",
    "    def __init__(self, input_dim=85, hidden_dim = [4,], out_dim = 10, p_dropout = 0.0):\n",
    "        assert len(hidden_dim)>0, \"at least one hidden layer\"\n",
    "        layers = [] #aqui guardamos una lista con todas las capas\n",
    "        #first layer\n",
    "        layers.append(Capa(input_dim,hidden_dim[0], p_dropout=p_dropout))\n",
    "        # Inserte el código para generar más capas ocultas si el número de elementos de hidden_dim > 1\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        for i in range(1, len(hidden_dim)):\n",
    "            layers.append(Capa(hidden_dim[i-1], hidden_dim[i], p_dropout=p_dropout))\n",
    "             \n",
    "        #Output layer\n",
    "        layers.append(nn.Linear(hidden_dim[-1], out_dim))\n",
    "        super().__init__(*layers) #inicializamos la clase padre con la lista de capas\n",
    "                \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b653e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprobacion\n",
    "my_logger.test(\"Multicapa\")\n",
    "\n",
    "\n",
    "red = Red(input_dim = 85, hidden_dim=[5,7,5])\n",
    "print(red)\n",
    "assert len(list(red.children())) == 4, \"No has creado el número de capas ocultas correctamente\"\n",
    "capas = list(red.children())\n",
    "assert capas[0].ff.out_features == 5, \"la primera capa tiene 5 neuronas\"\n",
    "assert capas[1].ff.out_features == 7, \"la segunda capa tiene 7 neuronas\"\n",
    "assert capas[2].ff.out_features == 5, \"la tercera capa tiene 5 neuronas\"\n",
    "assert capas[3].out_features == 10, \"la capa de salida tiene 10 neuronas\"\n",
    "assert capas[0].ff.in_features == 85, \"la primera capa tiene 5 neuronas\"\n",
    "assert capas[1].ff.in_features == 5, \"la segunda capa tiene 7 neuronas\"\n",
    "assert capas[2].ff.in_features == 7, \"la tercera capa tiene 5 neuronas\"\n",
    "assert capas[3].in_features == 5, \"la capa de salida tiene 10 neuronas\"\n",
    "\n",
    "my_logger.success(\"Multicapa\",2.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d663dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Entrenando la red\n",
    "\n",
    "En el fichero `train_net` se le proporciona las funciones para entrenar la red neuronal empleando la red y dataloaders que acaba de crear. \n",
    "\n",
    "Si examina el código mientras entrena las redes observará lo siguiente:\n",
    "* Función `train_epoch`, es la encargada de realizar una pasada (epoch) por los datos de entrenamiento e ir actualizando los pesos mediante el método de gradiente\n",
    "* Funtion `valid_epoch`, realiza una pasada sobre los datos de validación y obtiene las metricas de la red. En este caso los pesos no se actualizan y se mantienen fijos durante todo el proceso.\n",
    "* Función `train`, alterna llamadas a `train_epoch`y `valid_epoch` durante un numero de epochs. Guarda los resultados de las métricas durante el proceso de entrenamiento.\n",
    "\n",
    "A continuación se muestra como entrenar la red para un caso sencillo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaaeb11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from train_net import train\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 0.01\n",
    "batch_size = 512\n",
    "\n",
    "# Define network\n",
    "red1 = Red(input_dim = 85,hidden_dim=[4], p_dropout=0)\n",
    "\n",
    "print(red1)\n",
    "\n",
    "# Create Adam optimizer\n",
    "#optimizer = optim.SGD(red1.parameters(), lr=learning_rate, momentum =0.9)\n",
    "optimizer = optim.Adam(red1.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= True )\n",
    "\n",
    "\n",
    "val_dataloader =  DataLoader(valid_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= False,\n",
    "                              drop_last=False) # \n",
    "\n",
    "#En train si el numero de muestras del dataset no es multiplo del batch_size se desprecian unas pocas cada epoch\n",
    "#Para validar el ultimpo batch tendra seguramente menos muestras\n",
    "\n",
    "train_losses, val_losses, train_acc, val_acc = train(red1, loss_fn, train_dataloader,val_dataloader, optimizer, num_epochs, device='cpu')\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(train_losses, label='train loss')\n",
    "axes[0].plot(val_losses, label = 'val_loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(train_acc, label='train_acc')\n",
    "axes[1].plot(val_acc, label='val acc')\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef87d1ba",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Pruebas de entrenamiento\n",
    "\n",
    "En esta parte vamos a ir realizando distintos entrenamientos cambiando diferentes hiperparametros. \n",
    "\n",
    "### Efecto de la tasa de aprendizaje\n",
    "\n",
    "En este apartado veremos el efecto de subir excesivamente el valor de la tasas de aprendizaje (learning rate).\n",
    "Empleando la siguiente configuración:\n",
    "\n",
    "* Learning rate = 10\n",
    "* Numero de capas ocultas 1, neuronas 4\n",
    "* Epochs = 10\n",
    "* Batch size: 512\n",
    "* No dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dea441",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Completa con los valores que se indica para este apartado.\n",
    "learning_rate = 10\n",
    "batch_size = 512\n",
    "num_epochs = 10\n",
    "# Define network\n",
    "red = Red(hidden_dim=[4], p_dropout=0.0)\n",
    "\n",
    "# Create Adam optimizer\n",
    "#optimizer = optim.SGD(red1.parameters(), lr=learning_rate, momentum =0.9)\n",
    "optimizer = optim.Adam(red.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= True )\n",
    "\n",
    "\n",
    "val_dataloader =  DataLoader(valid_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= False,\n",
    "                              drop_last=False) # \n",
    "\n",
    "#En train si el numero de muestras del dataset no es multiplo del batch_size se desprecian unas pocas cada epoch\n",
    "#Para validar el ultimpo batch tendra seguramente menos muestras\n",
    "\n",
    "train_losses, val_losses, train_acc, val_acc = train(red, loss_fn, train_dataloader,val_dataloader, optimizer, num_epochs, device='cpu')\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(train_losses, label='train loss')\n",
    "axes[0].plot(val_losses, label = 'val_loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(train_acc, label='train_acc')\n",
    "axes[1].plot(val_acc, label='val acc')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd70c5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprobación \n",
    "my_logger.test(\"efecto lr\")\n",
    "# En este caso con un learning rate tan grande no converje y el accuraccy es muy bajo\n",
    "assert np.max(val_acc) == pytest.approx(0.12, abs=2e-2)\n",
    "my_logger.success(\"efecto lr\",1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6894ad7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Aumentar el número de epocas\n",
    "Veamos ahora el efecto de poner más o menos epocas en el entrenamiento empleando \n",
    "el learning rate del primer apartado. \n",
    "\n",
    "* Learning rate = 0.01\n",
    "* Numero de capas ocultas 1, neuronas 4\n",
    "* Epochs = 10\n",
    "* Batch size: 512\n",
    "* Sin dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b5a8e4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "learning_rate = 0.01\n",
    "batch_size = 512\n",
    "num_epochs = 10\n",
    "# Define network\n",
    "red = Red(hidden_dim=[4], p_dropout=0.0)\n",
    "\n",
    "# Create Adam optimizer\n",
    "#optimizer = optim.SGD(red.parameters(), lr=learning_rate, momentum =0.9)\n",
    "optimizer = optim.Adam(red.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= True )\n",
    "\n",
    "\n",
    "val_dataloader =  DataLoader(valid_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= False,\n",
    "                              drop_last=False) # \n",
    "\n",
    "#En train si el numero de muestras del dataset no es multiplo del batch_size se desprecian unas pocas cada epoch\n",
    "#Para validar el ultimpo batch tendra seguramente menos muestras\n",
    "\n",
    "train_losses, val_losses, train_acc, val_acc = train(red, loss_fn, train_dataloader,val_dataloader, optimizer, num_epochs, device='cpu')\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(train_losses, label='train loss')\n",
    "axes[0].plot(val_losses, label = 'val_loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(train_acc, label='train_acc')\n",
    "axes[1].plot(val_acc, label='val acc')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c36a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprobacion \n",
    "my_logger.test(\"efecto num_epoch\")\n",
    "\n",
    "# En este caso vemos que el algoritmo ya comienza a converger y que \n",
    "# El accuracy es mucho más alto, aunque da la sensacion de que con más\n",
    "# epochs seguiria subiendo \n",
    "assert np.max(val_acc) == pytest.approx(0.68, abs=1e-1)\n",
    "my_logger.success(\"efecto num_epoch\",1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eaa2df",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Ahora repetiremos pero poniendo más epocas:\n",
    "* Learning rate = 0.01\n",
    "* Numero de capas ocultas 1, neuronas 4\n",
    "* Epochs = 100\n",
    "* Batch size: 512\n",
    "* Sin dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c30ea1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "learning_rate = 0.01\n",
    "batch_size = 512\n",
    "num_epochs = 100\n",
    "# Define network\n",
    "red = Red(hidden_dim=[4], p_dropout=0.0)\n",
    "\n",
    "# Create Adam optimizer\n",
    "#optimizer = optim.SGD(red.parameters(), lr=learning_rate, momentum =0.9)\n",
    "optimizer = optim.Adam(red.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= True )\n",
    "\n",
    "\n",
    "val_dataloader =  DataLoader(valid_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= False,\n",
    "                              drop_last=False) # \n",
    "\n",
    "#En train si el numero de muestras del dataset no es multiplo del batch_size se desprecian unas pocas cada epoch\n",
    "#Para validar el ultimpo batch tendra seguramente menos muestras\n",
    "\n",
    "train_losses, val_losses, train_acc, val_acc = train(red, loss_fn, train_dataloader,val_dataloader, optimizer, num_epochs, device='cpu')\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(train_losses, label='train loss')\n",
    "axes[0].plot(val_losses, label = 'val_loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(train_acc, label='train_acc')\n",
    "axes[1].plot(val_acc, label='val acc')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029c79a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprobacion \n",
    "my_logger.test(\"efecto muchos num_epoch\")\n",
    "\n",
    "# Al aumentar los epoch, hemos conseguido mejorar unque da la impresion de que\n",
    "# pasado un tiempo satura el aprendizaje. El modelo que tenemos es demasiado pequeño \n",
    "# para esta tarea.\n",
    "assert np.max(val_acc) == pytest.approx(0.72, abs=4e-2)\n",
    "my_logger.success(\"efecto muchos num_epoch\",1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13733317",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Efecto del tamaño de batch\n",
    "Veremos ahora el efecto de tomar cambiar `batch_size`, mire los resultados\n",
    "de apartados anteriores para comparar.\n",
    "\n",
    "* Learning rate = 0.01\n",
    "* Numero de capas ocultas 1, neuronas 4\n",
    "* Epochs = 70\n",
    "* Batch size: 32\n",
    "* Sin dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a86dc2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "num_epochs = 70\n",
    "# Define network\n",
    "red = Red(hidden_dim=[4], p_dropout=0.0)\n",
    "\n",
    "# Create Adam optimizer\n",
    "optimizer = optim.SGD(red.parameters(), lr=learning_rate, momentum =0.9)\n",
    "#optimizer = optim.Adam(red1.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= True )\n",
    "\n",
    "\n",
    "val_dataloader =  DataLoader(valid_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= False,\n",
    "                              drop_last=False) # \n",
    "\n",
    "#En train si el numero de muestras del dataset no es multiplo del batch_size se desprecian unas pocas cada epoch\n",
    "#Para validar el ultimpo batch tendra seguramente menos muestras\n",
    "\n",
    "train_losses, val_losses, train_acc, val_acc = train(red, loss_fn, train_dataloader,val_dataloader, optimizer, num_epochs, device='cpu')\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(train_losses, label='train loss')\n",
    "axes[0].plot(val_losses, label = 'val_loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(train_acc, label='train_acc')\n",
    "axes[1].plot(val_acc, label='val acc')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2307d15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprobacion \n",
    "my_logger.test(\"efecto batchsize\")\n",
    "# El tiempo de entrenamiento es mucho mayor ahora ya que se realizan muchos mas pasos de gradiente\n",
    "assert np.max(val_acc) == pytest.approx(0.73, abs=2e-2)\n",
    "my_logger.success(\"efecto batchsize\",0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4085a8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Ahora subimos el tamaño del batch:\n",
    "* Learning rate = 0.01\n",
    "* Numero de capas ocultas 1, neuronas 4\n",
    "* Epochs = 70\n",
    "* Batch size: 256\n",
    "* Sin dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ec785",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "learning_rate = 0.01\n",
    "batch_size = 256\n",
    "num_epochs = 70\n",
    "# Define network\n",
    "red = Red(hidden_dim=[4], p_dropout=0.0)\n",
    "\n",
    "# Create Adam optimizer\n",
    "optimizer = optim.SGD(red.parameters(), lr=learning_rate, momentum =0.9)\n",
    "#optimizer = optim.Adam(red1.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= True )\n",
    "\n",
    "\n",
    "val_dataloader =  DataLoader(valid_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= False,\n",
    "                              drop_last=False) # \n",
    "\n",
    "#En train si el numero de muestras del dataset no es multiplo del batch_size se desprecian unas pocas cada epoch\n",
    "#Para validar el ultimpo batch tendra seguramente menos muestras\n",
    "\n",
    "train_losses, val_losses, train_acc, val_acc = train(red, loss_fn, train_dataloader,val_dataloader, optimizer, num_epochs, device='cpu')\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(train_losses, label='train loss')\n",
    "axes[0].plot(val_losses, label = 'val_loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(train_acc, label='train_acc')\n",
    "axes[1].plot(val_acc, label='val acc')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86515b64",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprobacion \n",
    "my_logger.test(\"efecto batch_size grande\")\n",
    "\n",
    "# El el resultado ha sido similar pero con menos iteraciones (más rapido)\n",
    "assert np.max(val_acc) == pytest.approx(0.717, abs=5e-2)\n",
    "my_logger.success(\"efecto batch_size grande\",0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33d827",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Ahora subimos el tamaño del batch a un valor extremadamente grande:\n",
    "* Learning rate = 0.01\n",
    "* Numero de capas ocultas 1, neuronas 4\n",
    "* Epochs = 70\n",
    "* Batch size: 1024\n",
    "* Sin dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87335f7c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "learning_rate = 0.01\n",
    "batch_size = 1024\n",
    "num_epochs = 70\n",
    "# Define network\n",
    "red = Red(hidden_dim=[4], p_dropout=0.0)\n",
    "\n",
    "# Create Adam optimizer\n",
    "optimizer = optim.SGD(red.parameters(), lr=learning_rate, momentum =0.9)\n",
    "#optimizer = optim.Adam(red1.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= True )\n",
    "\n",
    "\n",
    "val_dataloader =  DataLoader(valid_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= False,\n",
    "                              drop_last=False) # \n",
    "\n",
    "#En train si el numero de muestras del dataset no es multiplo del batch_size se desprecian unas pocas cada epoch\n",
    "#Para validar el ultimpo batch tendra seguramente menos muestras\n",
    "\n",
    "train_losses, val_losses, train_acc, val_acc = train(red, loss_fn, train_dataloader,val_dataloader, optimizer, num_epochs, device='cpu')\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(train_losses, label='train loss')\n",
    "axes[0].plot(val_losses, label = 'val_loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(train_acc, label='train_acc')\n",
    "axes[1].plot(val_acc, label='val acc')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f7e6f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprobacion \n",
    "my_logger.test(\"efecto batch_size muy grande\")\n",
    "\n",
    "# El tamaño de batch es tan grande que con 70 epochs aun no ha terminado de entrenar porque tenemos pocas iteraciones\n",
    "# Si repetieramos la prueba con más epochs llegariamos a resultados similares a los anteriores\n",
    "assert np.max(val_acc) == pytest.approx(0.66, abs=4e-1)\n",
    "my_logger.success(\"efecto batch_size muy grande\",0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c25dfa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**NOTA**: De los apartados anteriores parece deducirse que al aumentar el tamaño del batch el resultado empeora. Pero es una conclusion no del todo adecuada. Tenga en cuenta que al aumentar el tamaño del batch hay menos iteraciones del optimizador por epoch (los pesos de la red se actualizan menos veces) y lo que ocurre a la vista de las gráficas anteriores es que quizas deberiamos aumentar el número de epochs en este último caso para que le de tiempo a la red a converger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a66357",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Cambio estructura de la red. \n",
    "\n",
    "Ahora probaremos el efecto de variar el número de neuronas y de capas. Comenzaremos aumentando el número de capas.\n",
    "\n",
    "Ahora subimos el tamaño:\n",
    "* Learning rate = 0.01\n",
    "* Numero de capas ocultas 2, neuronas 4,4\n",
    "* Epochs = 100\n",
    "* Batch size: 512\n",
    "* Sin dropout\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64efda03",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "learning_rate = 0.01\n",
    "batch_size = 512\n",
    "num_epochs = 100\n",
    "# Define network\n",
    "red = Red(hidden_dim=[4, 4], p_dropout=0.0)\n",
    "\n",
    "# Create Adam optimizer\n",
    "optimizer = optim.SGD(red.parameters(), lr=learning_rate, momentum =0.9)\n",
    "#optimizer = optim.Adam(red1.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= True )\n",
    "\n",
    "\n",
    "val_dataloader =  DataLoader(valid_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= False,\n",
    "                              drop_last=False) # \n",
    "\n",
    "#En train si el numero de muestras del dataset no es multiplo del batch_size se desprecian unas pocas cada epoch\n",
    "#Para validar el ultimpo batch tendra seguramente menos muestras\n",
    "\n",
    "train_losses, val_losses, train_acc, val_acc = train(red, loss_fn, train_dataloader,val_dataloader, optimizer, num_epochs, device='cpu')\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(train_losses, label='train loss')\n",
    "axes[0].plot(val_losses, label = 'val_loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(train_acc, label='train_acc')\n",
    "axes[1].plot(val_acc, label='val acc')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba3835",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprobacion \n",
    "my_logger.test(\"efecto pocas neuronas\")\n",
    "\n",
    "# Resultado similar al anterior, tenemos muy pocas neuronas\n",
    "assert np.max(val_acc) == pytest.approx(0.7, abs=2e-1)\n",
    "my_logger.success(\"efecto pocas neuronas\",0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ecc0fe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Ahora subimos el tamaño:\n",
    "* Learning rate = 0.01\n",
    "* Numero de capas ocultas 1, neuronas 40\n",
    "* Epochs = 100\n",
    "* Batch size: 512\n",
    "* Sin dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a13a2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "learning_rate = 0.01\n",
    "batch_size = 512\n",
    "num_epochs = 100\n",
    "# Define network\n",
    "red = Red(hidden_dim=[40], p_dropout=0.0)\n",
    "\n",
    "# Create Adam optimizer\n",
    "optimizer = optim.SGD(red.parameters(), lr=learning_rate, momentum =0.9)\n",
    "#optimizer = optim.Adam(red1.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= True )\n",
    "\n",
    "\n",
    "val_dataloader =  DataLoader(valid_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= False,\n",
    "                              drop_last=False) # \n",
    "\n",
    "#En train si el numero de muestras del dataset no es multiplo del batch_size se desprecian unas pocas cada epoch\n",
    "#Para validar el ultimpo batch tendra seguramente menos muestras\n",
    "\n",
    "train_losses, val_losses, train_acc, val_acc = train(red, loss_fn, train_dataloader,val_dataloader, optimizer, num_epochs, device='cpu')\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(train_losses, label='train loss')\n",
    "axes[0].plot(val_losses, label = 'val_loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(train_acc, label='train_acc')\n",
    "axes[1].plot(val_acc, label='val acc')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b334ae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprobacion \n",
    "my_logger.test(\"efecto mas neuronas\")\n",
    "\n",
    "# ahora si que vemos una mejora significativa en validacion\n",
    "# Tambien observamos que el modelo tiene mucha más capacidad. En training va mucho mejor que en validacion\n",
    "assert np.max(val_acc) == pytest.approx(0.838, abs=2e-2)\n",
    "my_logger.success(\"efecto mas neuronas\",0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d11c98d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Ahora subimos el tamaño y el número de capas:\n",
    "* Learning rate = 0.01\n",
    "* Numero de capas ocultas 2, neuronas 40, 40\n",
    "* Epochs = 100\n",
    "* Batch size: 512\n",
    "* Sin dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea0fce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "learning_rate = 0.01\n",
    "batch_size = 512\n",
    "num_epochs = 100\n",
    "# Define network\n",
    "red = Red(hidden_dim=[40, 40], p_dropout=0.0)\n",
    "\n",
    "# Create Adam optimizer\n",
    "optimizer = optim.SGD(red.parameters(), lr=learning_rate, momentum =0.9)\n",
    "#optimizer = optim.Adam(red1.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= True )\n",
    "\n",
    "\n",
    "val_dataloader =  DataLoader(valid_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= False,\n",
    "                              drop_last=False) # \n",
    "\n",
    "#En train si el numero de muestras del dataset no es multiplo del batch_size se desprecian unas pocas cada epoch\n",
    "#Para validar el ultimpo batch tendra seguramente menos muestras\n",
    "\n",
    "train_losses, val_losses, train_acc, val_acc = train(red, loss_fn, train_dataloader,val_dataloader, optimizer, num_epochs, device='cpu')\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(train_losses, label='train loss')\n",
    "axes[0].plot(val_losses, label = 'val_loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(train_acc, label='train_acc')\n",
    "axes[1].plot(val_acc, label='val acc')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750b1db",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprobacion \n",
    "my_logger.test(\"efecto mas capas y neuronas\")\n",
    "\n",
    "# el resultado es similar al anterior, pero observamos overfitting el modelo es muy muy potente\n",
    "# El val_loss comienza a aumentar despues del epoch 20\n",
    "assert np.max(val_acc) == pytest.approx(0.83, abs=2e-2)\n",
    "my_logger.success(\"efecto mas capas y neuronas\",0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df713f14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Regularizacion\n",
    "\n",
    "Para reducir el overfitting del caso anterior vamos a añadir regularización. En este caso vamos a añadir dropout \n",
    "\n",
    "Ahora subimos el tamaño y el número de capas:\n",
    "* Learning rate = 0.01\n",
    "* Numero de capas ocultas 2, neuronas 40, 40\n",
    "* Epochs = 200\n",
    "* Batch size: 512\n",
    "* p_dropout = 0.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efeb242",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "learning_rate = 0.01\n",
    "batch_size = 512\n",
    "num_epochs = 200\n",
    "# Define network\n",
    "red = Red(hidden_dim=[40, 40], p_dropout=0.25)\n",
    "\n",
    "# Create Adam optimizer\n",
    "optimizer = optim.SGD(red.parameters(), lr=learning_rate, momentum =0.9)\n",
    "#optimizer = optim.Adam(red1.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= True )\n",
    "\n",
    "\n",
    "val_dataloader =  DataLoader(valid_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= False,\n",
    "                              drop_last=False) # \n",
    "\n",
    "#En train si el numero de muestras del dataset no es multiplo del batch_size se desprecian unas pocas cada epoch\n",
    "#Para validar el ultimpo batch tendra seguramente menos muestras\n",
    "\n",
    "train_losses, val_losses, train_acc, val_acc = train(red, loss_fn, train_dataloader,val_dataloader, optimizer, num_epochs, device='cpu')\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(train_losses, label='train loss')\n",
    "axes[0].plot(val_losses, label = 'val_loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(train_acc, label='train_acc')\n",
    "axes[1].plot(val_acc, label='val acc')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586f510",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprobacion \n",
    "my_logger.test(\"efecto dropout\")\n",
    "\n",
    "# el dropout reduce el overfitting y ademas mejora el resultado de validacion\n",
    "assert np.max(val_acc) == pytest.approx(0.854, abs=2e-2)\n",
    "my_logger.success(\"efecto dropout\",0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e58b71c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "El ultimo modelo aumentaremos el numero de neuronas y mantendremos el dropout, ademas de subir el número de epoch\n",
    "* Learning rate = 0.01\n",
    "* Numero de capas ocultas 2, neuronas 100, 100\n",
    "* Epochs = 400\n",
    "* Batch size: 512\n",
    "* p_dropout = 0.25\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897c031",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "learning_rate = 0.01\n",
    "batch_size = 512\n",
    "num_epochs = 400\n",
    "# Define network\n",
    "red = Red(hidden_dim=[100, 100], p_dropout=0.25)\n",
    "\n",
    "# Create Adam optimizer\n",
    "optimizer = optim.SGD(red.parameters(), lr=learning_rate, momentum =0.9)\n",
    "#optimizer = optim.Adam(red1.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= True )\n",
    "\n",
    "\n",
    "val_dataloader =  DataLoader(valid_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle= False,\n",
    "                              drop_last=False) # \n",
    "\n",
    "#En train si el numero de muestras del dataset no es multiplo del batch_size se desprecian unas pocas cada epoch\n",
    "#Para validar el ultimpo batch tendra seguramente menos muestras\n",
    "\n",
    "train_losses, val_losses, train_acc, val_acc = train(red, loss_fn, train_dataloader,val_dataloader, optimizer, num_epochs, device='cuda')\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(train_losses, label='train loss')\n",
    "axes[0].plot(val_losses, label = 'val_loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(train_acc, label='train_acc')\n",
    "axes[1].plot(val_acc, label='val acc')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f33006d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprobacion \n",
    "my_logger.test(\"efecto dropout y red mas grande\")\n",
    "\n",
    "# finalmente con dropout podemos subir la complejidad del modelo y aun mejora un poco mas\n",
    "assert np.max(val_acc) == pytest.approx(0.861, abs=2e-2)\n",
    "my_logger.success(\"efecto dropout y red mas grande\",0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ff1dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Prueba con grabación\n",
    "\n",
    "Tomaremos el modelo del apartado anterior para hacer nuestras pruebas con audios grabados\n",
    "al instante.\n",
    "\n",
    "Lo primero que haremos es  completar las funciones con el código que **realizaste en la práctica 4** para la extracción de características\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0cd63",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywebrtc import AudioRecorder, CameraStream\n",
    "from IPython.display import Audio\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f62114",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  COPIA EL CODIGO DE LA PRACTICA 4 en las siguientes funciones\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "from utils import detect_speech\n",
    "\n",
    "def spectral_centroid(S,f):\n",
    "    \"\"\" Arguments:\n",
    "        S: Espectrogram (complex matrix obtained using stft)\n",
    "        f: frequencies vector, vectorf with the corresponding frequency of each row of S\n",
    "        Returns:\n",
    "           Vector with the spectral centroid for all frames\n",
    "\n",
    "        NOTE: use of librosa.feature.spectral_centroid is not allowed\n",
    "    \"\"\"\n",
    "    eps = 1e-10\n",
    "    S = abs(S) # Tomamos valor absoluto\n",
    "\n",
    "    numerador = np.sum(S*f[:, None], axis=0)\n",
    "    denominador = np.sum(S, axis=0) + eps\n",
    "    \n",
    "    sc = numerador/denominador # Guarde aqui el vector centroide espectral\n",
    "    \n",
    "    return sc\n",
    "    \n",
    "def spectral_flux(S):\n",
    "    S = np.abs(S) # nos aseguramos de tomar la magnitud\n",
    "    \n",
    "    flux = np.zeros(S.shape[1])\n",
    "\n",
    "    #diferencia de cada trama consecutiva\n",
    "    diff = S[:, 1:] - S[:, :-1]\n",
    "\n",
    "    #\n",
    "    flux[1:] = np.sum(diff**2, axis=0)\n",
    "    \n",
    "    return flux\n",
    "\n",
    "def spectral_spread(S, f, centroid = None):\n",
    "    S = np.abs(S)\n",
    "    eps = 1e-10\n",
    "    if centroid is None:\n",
    "        centroid = spectral_centroid(S,f)\n",
    "\n",
    "    centroid = centroid.reshape(1,-1)\n",
    "    f = f.reshape(-1,1)\n",
    "    \n",
    "    desv_f = (f - centroid)**2\n",
    "\n",
    "    spread = np.sqrt((desv_f * S).sum(axis=0) / (S.sum(axis=0)+eps))\n",
    "    return spread\n",
    "\n",
    "def spectral_skewness(S,f, centroid = None, spread = None):\n",
    "    S = np.abs(S)\n",
    "    eps = 1e-10\n",
    "    if centroid is None:\n",
    "        centroid = spectral_centroid(S,f)\n",
    "\n",
    "    if spread is None:\n",
    "        spread = spectral_spread(S,f,centroid=centroid)\n",
    "\n",
    "    centroid = centroid.reshape(1,-1)\n",
    "    f = f.reshape(-1,1)\n",
    " \n",
    "    desv_f = f - centroid\n",
    "\n",
    "    num = ((desv_f ** 3)*S).sum(axis=0)\n",
    "    den = spread**3 * (S.sum(axis=0))\n",
    "    return num / (den + eps)\n",
    "\n",
    "def spectral_kurtosis(S,f, centroid = None, spread = None):\n",
    "    S = np.abs(S)\n",
    "    eps = 1e-10\n",
    "    if centroid is None:\n",
    "        centroid = spectral_centroid(S,f)\n",
    "\n",
    "    if spread is None:\n",
    "        spread = spectral_spread(S,f,centroid=centroid)\n",
    "\n",
    "    centroid = centroid.reshape(1,-1)\n",
    "    f = f.reshape(-1,1)\n",
    " \n",
    "    desv_f = f - centroid\n",
    "\n",
    "    num = ((desv_f ** 4)*S).sum(axis=0)\n",
    "    den = spread**4 * (S.sum(axis=0))\n",
    "    return num / (den + eps)\n",
    "    \n",
    "def extract_features(audio_data, fs):\n",
    "    win_length = round(0.032*fs)\n",
    "    hop_length = win_length // 2\n",
    "    window = 'hamming'\n",
    "    n_fft = 512\n",
    "    n_mels = 40\n",
    "    n_mfcc=13\n",
    "    f = librosa.core.fft_frequencies(n_fft=n_fft, sr = fs)\n",
    "\n",
    "\n",
    "    audio_mask = detect_speech(audio_data, fs)\n",
    "    audio_trim = audio_data[np.where(audio_mask)]\n",
    "    \n",
    "    S = librosa.stft(audio_trim, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window)\n",
    "    absS = np.abs(S)\n",
    "\n",
    "    Smel = librosa.feature.melspectrogram(S= absS ,sr=fs,  n_fft=n_fft, \n",
    "                                      hop_length=hop_length, \n",
    "                                      win_length=win_length,n_mels = n_mels,\n",
    "                                      ) # reutilizamos el espectrograma lineal\n",
    "    \n",
    "    # Extract MFCCs (Mel-frequency cepstral coefficients)\n",
    "    mfcc = librosa.feature.mfcc(S = librosa.power_to_db(Smel), sr=fs,  n_mfcc=n_mfcc)\n",
    "\n",
    "    \n",
    "    # Calculate delta MFCCs\n",
    "    \n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    \n",
    "    # Calculate spectral flux\n",
    "    flux = spectral_flux(absS)\n",
    "\n",
    "    # Calculate spectral centroid\n",
    "    centroid = spectral_centroid(absS,f)\n",
    "\n",
    "    # Calculate spectral spread\n",
    "    spread = spectral_spread(absS,f,centroid=centroid)\n",
    "\n",
    "    # Calculate spectral skewness\n",
    "    skewness = spectral_skewness(absS,f,centroid=centroid, spread=spread)\n",
    "\n",
    "    # Calculate spectral kurtosis\n",
    "    kurtosis = spectral_kurtosis(absS,f,centroid=centroid, spread=spread)\n",
    "\n",
    "    # Calculate spectral rolloff point\n",
    "    rolloff = librosa.feature.spectral_rolloff(S=absS, sr=fs,  roll_percent=0.85)\n",
    "\n",
    "    audio_features = {}\n",
    "    audio_features['mfcc']=mfcc\n",
    "    audio_features['mfcc_delta']=mfcc_delta\n",
    "    audio_features['flux']=flux\n",
    "    audio_features['centroid']=centroid\n",
    "    audio_features['spread']=spread\n",
    "    audio_features['skewness']=skewness\n",
    "    audio_features['kurtosis']=kurtosis\n",
    "    audio_features['rolloff']=rolloff\n",
    "\n",
    "    audio_features = vectorize_features(audio_features)\n",
    "    \n",
    "    return audio_features\n",
    "\n",
    "\n",
    "def vectorize_features(audio_features):\n",
    "    #13 features\n",
    "    avg_mfcc = np.mean(audio_features['mfcc'],axis=1)\n",
    "    #13 features\n",
    "    var_mfcc = np.std(audio_features['mfcc'],axis=1)\n",
    "    #13 features\n",
    "    max_mfcc = np.max(audio_features['mfcc'],axis=1)\n",
    "    #13 features\n",
    "    min_mfcc = np.min(audio_features['mfcc'],axis=1)\n",
    "    #13 features\n",
    "    avg_mfcc_delta = np.mean(audio_features['mfcc_delta'],axis=1)\n",
    "    #13 features\n",
    "    var_mfcc_delta = np.std(audio_features['mfcc_delta'],axis=1)\n",
    "\n",
    "    #2 features\n",
    "    avg_flux = np.mean(audio_features['flux'])\n",
    "    var_flux = np.std(audio_features['flux'])\n",
    "\n",
    "    #5 feature\n",
    "    avg_centroid = np.mean(audio_features['centroid'])\n",
    "    avg_spread = np.mean(audio_features['spread'])\n",
    "    avg_skewness = np.mean(audio_features['skewness'])\n",
    "    avg_kurtosis = np.mean(audio_features['kurtosis'])\n",
    "    avg_rolloff = np.mean(audio_features['rolloff'])\n",
    "\n",
    "                    \n",
    "\n",
    "    return np.concatenate([avg_mfcc, var_mfcc, \n",
    "                           max_mfcc, min_mfcc, \n",
    "                           avg_mfcc_delta, var_mfcc_delta,\n",
    "                           [avg_flux, var_flux],\n",
    "                           [avg_centroid, avg_spread, avg_skewness, avg_kurtosis, avg_rolloff]])\n",
    "                           \n",
    "\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eed6f5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bddf5d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "* Ejecuta la siguiente celda.\n",
    "* Pulsa circulo negro para comenzar a grabar.\n",
    "* Di un numero\n",
    "* Pulsa de nuevo para terminar la grabacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc52a8d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "camera = CameraStream(constraints={'audio': True,'video':False})\n",
    "recorder = AudioRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd4234",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convertimos a wav\n",
    "with open('recording.webm', 'wb') as f:\n",
    "    f.write(recorder.audio.value)\n",
    "!ffmpeg -i recording.webm -ac 1 -ar 16000 -f wav file.wav  -y -hide_banner -loglevel panic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f97d6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Escucha para comprobar que todo sigue ok\n",
    "Audio('file.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e0b39",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pintamos para ver el aspecto de la señal  que no sature \n",
    "audio_data, fs = librosa.load('file.wav', sr=None)\n",
    "t = np.arange(len(audio_data))/fs\n",
    "plt.plot(t,audio_data)\n",
    "_ = plt.xlabel('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd651d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_feat = extract_features(audio_data, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177a16d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_feat_norm = (audio_feat.reshape(1,-1) - m_train)/ s_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0c616",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "red.eval()\n",
    "x = torch.from_numpy(audio_feat_norm.astype(np.float32)).to('cuda')\n",
    "y = red(x)\n",
    "print(\"Has dicho: \", torch.argmax(y).item()) # clasificamos con el máximo"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.366423,
   "end_time": "2025-06-13T12:55:29.818478",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-13T12:55:15.452055",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
